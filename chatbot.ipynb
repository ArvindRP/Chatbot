{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j0iGWyioq_M1",
        "outputId": "64d864d3-ae3b-491d-f366-06159fe3db1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
        "import pandas as pd\n",
        "\n",
        "tf.logging.set_verbosity( tf.logging.ERROR ) # Just to remove warnings!\n",
        "\n",
        "print( tf.VERSION )\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i0dU-tAsrWXS",
        "colab": {}
      },
      "source": [
        "#preparing the dataset extracted from whatsapp chat\n",
        "with open('file3.txt', 'w') as file3:\n",
        "    with open('train.from', 'r') as file1:\n",
        "        with open('train.to', 'r') as file2:\n",
        "            for line1, line2 in zip(file1, file2):\n",
        "                print(line1.strip() +'\\t'+ line2.strip(), file=file3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UwtLfDZAr5eN",
        "outputId": "39f1a31f-0853-4c41-e65c-1cc9841e1d61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "lines = pd.read_table( 'file3.txt' , names=[ 'user1' , 'user2' ] )\n",
        "lines = lines.iloc[0:1800] # choose the number of conversation on which you want to train your dataset\n",
        "\n",
        "# lines = line.iloc[:] \n",
        "# uncomment the above line to train on whole dataset\n",
        "\n",
        "lines.head(20)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user1</th>\n",
              "      <th>user2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HI</td>\n",
              "      <td>hello</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>how are you?</td>\n",
              "      <td>Good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>good morning</td>\n",
              "      <td>good morning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>good night</td>\n",
              "      <td>good night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>where are you now</td>\n",
              "      <td>Home</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>what are you doing</td>\n",
              "      <td>Nothing ,just some college stuff</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>who would win in a fight, lebron james, or a g...</td>\n",
              "      <td>Depends if those sandles are fitted with Nike ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The power of a charismatic lead.  Plus the sho...</td>\n",
              "      <td>Agreed, Ioan Gruffudd intro's and outro's for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I don't know about you, but I don't typically ...</td>\n",
              "      <td>I remember when I was in high school I had a h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>I'm saying the people screaming racist on a mo...</td>\n",
              "      <td>Oh, I thought you were saying it the other way...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>:|==========כ newlinechar  newlinechar That lo...</td>\n",
              "      <td>haha props for the effort</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Sure sure, but it's a first name and a low res...</td>\n",
              "      <td>Doesn't matter. It's bad form.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Hnnng Lundqvist. Great player, and uhm...is th...</td>\n",
              "      <td>[No, but there's this.](http://www.underwearex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>You could learn welding and/or machining. Both...</td>\n",
              "      <td>Both of those are on the top of my list. I've ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>I like using Spotify (be sure to use the web p...</td>\n",
              "      <td>For the windows app there are tools to auto mu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Actually he only said medical marijuana in 2001.</td>\n",
              "      <td>Very true, entirely my bad on implying it was ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Psst,  if you want him to win, try being more ...</td>\n",
              "      <td>My optimism was defeated when I hoped for chan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Kiper fucking had him to us at 9 in his latest...</td>\n",
              "      <td>Hahahaha...didn't see that. He gets paid to lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Lol Gurley was NOT gonna fall to 23</td>\n",
              "      <td>I know, everyone had him all over their mocks,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>you can turn off subreddit style</td>\n",
              "      <td>thats for puss-puss</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                user1                                              user2\n",
              "0                                                  HI                                              hello\n",
              "1                                        how are you?                                               Good\n",
              "2                                        good morning                                       good morning\n",
              "3                                          good night                                         good night\n",
              "4                                   where are you now                                               Home\n",
              "5                                  what are you doing                   Nothing ,just some college stuff\n",
              "6   who would win in a fight, lebron james, or a g...  Depends if those sandles are fitted with Nike ...\n",
              "7   The power of a charismatic lead.  Plus the sho...  Agreed, Ioan Gruffudd intro's and outro's for ...\n",
              "8   I don't know about you, but I don't typically ...  I remember when I was in high school I had a h...\n",
              "9   I'm saying the people screaming racist on a mo...  Oh, I thought you were saying it the other way...\n",
              "10  :|==========כ newlinechar  newlinechar That lo...                          haha props for the effort\n",
              "11  Sure sure, but it's a first name and a low res...                     Doesn't matter. It's bad form.\n",
              "12  Hnnng Lundqvist. Great player, and uhm...is th...  [No, but there's this.](http://www.underwearex...\n",
              "13  You could learn welding and/or machining. Both...  Both of those are on the top of my list. I've ...\n",
              "14  I like using Spotify (be sure to use the web p...  For the windows app there are tools to auto mu...\n",
              "15   Actually he only said medical marijuana in 2001.  Very true, entirely my bad on implying it was ...\n",
              "16  Psst,  if you want him to win, try being more ...  My optimism was defeated when I hoped for chan...\n",
              "17  Kiper fucking had him to us at 9 in his latest...  Hahahaha...didn't see that. He gets paid to lo...\n",
              "18                Lol Gurley was NOT gonna fall to 23  I know, everyone had him all over their mocks,...\n",
              "19                   you can turn off subreddit style                                thats for puss-puss"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mNjLtQyoswDg",
        "outputId": "5ae309c3-6018-44b8-d401-ed25ec85419d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# preparing data for the encoder\n",
        "\n",
        "user1_lines = list()\n",
        "for line in lines.user1:\n",
        "    user1_lines.append( line ) \n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( user1_lines ) \n",
        "tokenized_user1_lines = tokenizer.texts_to_sequences( user1_lines ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_user1_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_input_length = np.array( length_list ).max()\n",
        "# print( 'user1 max length is {}'.format( max_input_length ))\n",
        "\n",
        "padded_user1_lines = preprocessing.sequence.pad_sequences( tokenized_user1_lines , maxlen=max_input_length , padding='post' )\n",
        "encoder_input_data = np.array( padded_user1_lines )\n",
        "# print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n",
        "\n",
        "user1_word_dict = tokenizer.word_index\n",
        "num_user1_tokens = len( user1_word_dict )+1\n",
        "# print( 'Number of user1 tokens = {}'.format( num_user1_tokens))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user1 max length is 77\n",
            "Encoder input data shape -> (1800, 77)\n",
            "Number of user1 tokens = 5941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jVoiJ5kTtn5Q",
        "outputId": "c369f1f4-f3d6-49dc-dca2-e66fbe66bc0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#preparing the input data for decoder \n",
        "\n",
        "user2_lines = list()\n",
        "for line in lines.user2:\n",
        "    user2_lines.append( '<START> ' + line + ' <END>' )  #appending start and end tag \n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( user2_lines ) \n",
        "tokenized_user2_lines = tokenizer.texts_to_sequences( user2_lines ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_user2_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_output_length = np.array( length_list ).max()\n",
        "# print( 'user2 max length is {}'.format( max_output_length ))\n",
        "\n",
        "padded_user2_lines = preprocessing.sequence.pad_sequences( tokenized_user2_lines , maxlen=max_output_length, padding='post' )\n",
        "decoder_input_data = np.array( padded_user2_lines )\n",
        "# print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
        "\n",
        "user2_word_dict = tokenizer.word_index\n",
        "num_user2_tokens = len( user2_word_dict )+1\n",
        "# print( 'Number of user2 tokens = {}'.format( num_user2_tokens))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user2 max length is 71\n",
            "Decoder input data shape -> (1800, 71)\n",
            "Number of user2 tokens = 5690\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q71oBgsFtoGW",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_7Etzl60toR2",
        "outputId": "09b609ad-4887-44b9-a3d2-cb20cde45a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#preparing the target data for decoder\n",
        "\n",
        "decoder_target_data = list()\n",
        "for token_seq in tokenized_user2_lines:\n",
        "    decoder_target_data.append( token_seq[ 1 : ] ) \n",
        "    \n",
        "padded_user2_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
        "onehot_user2_lines = utils.to_categorical( padded_user2_lines , num_user2_tokens )\n",
        "decoder_target_data = np.array( onehot_user2_lines )\n",
        "# print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder target data shape -> (1800, 71, 5690)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WPx9ogIxuAM4",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hviInJhyuAas",
        "outputId": "cd238f1f-a63a-4546-f0e9-e91110f34c1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "#defining the model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=( None , )) \n",
        "encoder_embedding = tf.keras.layers.Embedding( num_user1_tokens, 256 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 128 , return_state=True  )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( num_user2_tokens, 256 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 128 , return_state=True , return_sequences=True)\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( num_user2_tokens , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "filepath = \"model.h5\"\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint1]\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 256)    1520896     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 256)    1456640     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 128), (None, 197120      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 128),  197120      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 5690)   734010      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 4,105,786\n",
            "Trainable params: 4,105,786\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3cQBO3Rcujft",
        "outputId": "9f4ab222-c151-4c7c-a599-31d314be5745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#training the model\n",
        "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=100, epochs=50 ) \n",
        "model.save( 'model.h5' ) \n",
        "\n",
        "#comment this cell to directly load the processed model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1800 samples\n",
            "Epoch 1/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.5474\n",
            "Epoch 2/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.5228\n",
            "Epoch 3/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.5008\n",
            "Epoch 4/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.4822\n",
            "Epoch 5/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.4655\n",
            "Epoch 6/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.4511\n",
            "Epoch 7/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.4382\n",
            "Epoch 8/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.4251\n",
            "Epoch 9/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.4124\n",
            "Epoch 10/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.4001\n",
            "Epoch 11/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.3873\n",
            "Epoch 12/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.3754\n",
            "Epoch 13/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.3627\n",
            "Epoch 14/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.3506\n",
            "Epoch 15/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.3385\n",
            "Epoch 16/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.3265\n",
            "Epoch 17/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.3140\n",
            "Epoch 18/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.3019\n",
            "Epoch 19/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.2897\n",
            "Epoch 20/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.2774\n",
            "Epoch 21/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.2654\n",
            "Epoch 22/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.2532\n",
            "Epoch 23/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.2412\n",
            "Epoch 24/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.2294\n",
            "Epoch 25/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.2173\n",
            "Epoch 26/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.2053\n",
            "Epoch 27/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.1937\n",
            "Epoch 28/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.1814\n",
            "Epoch 29/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.1694\n",
            "Epoch 30/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.1576\n",
            "Epoch 31/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 1.1457\n",
            "Epoch 32/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.1332\n",
            "Epoch 33/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 1.1220\n",
            "Epoch 34/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 1.1097\n",
            "Epoch 35/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 1.0977\n",
            "Epoch 36/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 1.0857\n",
            "Epoch 37/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 1.0735\n",
            "Epoch 38/50\n",
            "1800/1800 [==============================] - 49s 27ms/sample - loss: 1.0619\n",
            "Epoch 39/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 1.0502\n",
            "Epoch 40/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 1.0381\n",
            "Epoch 41/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 1.0266\n",
            "Epoch 42/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 1.0150\n",
            "Epoch 43/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 1.0027\n",
            "Epoch 44/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 0.9919\n",
            "Epoch 45/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 0.9801\n",
            "Epoch 46/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 0.9690\n",
            "Epoch 47/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 0.9578\n",
            "Epoch 48/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 0.9472\n",
            "Epoch 49/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 0.9350\n",
            "Epoch 50/50\n",
            "1800/1800 [==============================] - 48s 27ms/sample - loss: 0.9244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyh6PWZyeuqE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83bb123b-333a-4547-d1e1-3faa3364ab9e"
      },
      "source": [
        "#loading the save model \n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "tf.keras.models.load_model('model.h5')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.training.Model at 0x7fa43e09b860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RfSGblw4ujrp",
        "colab": {}
      },
      "source": [
        "# defining the inference model \n",
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 128 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 128 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuZTQyfkertu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NIv84x2Euj1s",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YQ390W19uj0B",
        "colab": {}
      },
      "source": [
        "\n",
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( user1_word_dict[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OsoTkcHEujx9",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ruDbXDipujvZ",
        "outputId": "3c1590db-4542-479d-a634-659dcd28f0a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for epoch in range( encoder_input_data.shape[0] ):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter user1 sentence : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = user2_word_dict['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in user2_word_dict.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter user1 sentence : Good morning\n",
            " no problem end\n",
            "Enter user1 sentence : how are you doing\n",
            " i don't know it end\n",
            "Enter user1 sentence : why is that so\n",
            " no problem end\n",
            "Enter user1 sentence : tell me something\n",
            " end\n",
            "Enter user1 sentence : why are you not answering\n",
            " no problem no is a few com r redskins comments 1 newlinechar newlinechar newlinechar newlinechar newlinechar newlinechar newlinechar newlinechar newlinechar newlinechar newlinechar newlinechar newlinechar newlinechar i have a few of the few part of the most of the game end\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-10486e0f3b0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter user1 sentence : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser2_word_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ArWRiKxqujpO",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
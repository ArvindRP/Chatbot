{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "id": "j0iGWyioq_M1",
    "outputId": "e9776227-29ec-4f03-fd3c-ef6827629620"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
    "import pandas as pd\n",
    "\n",
    "tf.logging.set_verbosity( tf.logging.ERROR ) # Just to remove warnings!\n",
    "\n",
    "print( tf.VERSION )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0dU-tAsrWXS"
   },
   "outputs": [],
   "source": [
    "with open('file3.txt', 'w') as file3:\n",
    "    with open('train.from', 'r') as file1:\n",
    "        with open('train.to', 'r') as file2:\n",
    "            for line1, line2 in zip(file1, file2):\n",
    "                print(line1.strip() +'\\t'+ line2.strip(), file=file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "colab_type": "code",
    "id": "UwtLfDZAr5eN",
    "outputId": "de0e5013-940d-412e-8d50-d6f253313e5f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user1</th>\n",
       "      <th>user2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HI</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how are you?</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good morning</td>\n",
       "      <td>good morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good night</td>\n",
       "      <td>good night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>where are you now</td>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>what are you doing</td>\n",
       "      <td>Nothing ,just some college stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>who would win in a fight, lebron james, or a g...</td>\n",
       "      <td>Depends if those sandles are fitted with Nike ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The power of a charismatic lead.  Plus the sho...</td>\n",
       "      <td>Agreed, Ioan Gruffudd intro's and outro's for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I don't know about you, but I don't typically ...</td>\n",
       "      <td>I remember when I was in high school I had a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm saying the people screaming racist on a mo...</td>\n",
       "      <td>Oh, I thought you were saying it the other way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>:|==========כ newlinechar  newlinechar That lo...</td>\n",
       "      <td>haha props for the effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sure sure, but it's a first name and a low res...</td>\n",
       "      <td>Doesn't matter. It's bad form.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hnnng Lundqvist. Great player, and uhm...is th...</td>\n",
       "      <td>[No, but there's this.](http://www.underwearex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>You could learn welding and/or machining. Both...</td>\n",
       "      <td>Both of those are on the top of my list. I've ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I like using Spotify (be sure to use the web p...</td>\n",
       "      <td>For the windows app there are tools to auto mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Actually he only said medical marijuana in 2001.</td>\n",
       "      <td>Very true, entirely my bad on implying it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Psst,  if you want him to win, try being more ...</td>\n",
       "      <td>My optimism was defeated when I hoped for chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kiper fucking had him to us at 9 in his latest...</td>\n",
       "      <td>Hahahaha...didn't see that. He gets paid to lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lol Gurley was NOT gonna fall to 23</td>\n",
       "      <td>I know, everyone had him all over their mocks,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>you can turn off subreddit style</td>\n",
       "      <td>thats for puss-puss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                user1                                              user2\n",
       "0                                                  HI                                              hello\n",
       "1                                        how are you?                                               Good\n",
       "2                                        good morning                                       good morning\n",
       "3                                          good night                                         good night\n",
       "4                                   where are you now                                               Home\n",
       "5                                  what are you doing                   Nothing ,just some college stuff\n",
       "6   who would win in a fight, lebron james, or a g...  Depends if those sandles are fitted with Nike ...\n",
       "7   The power of a charismatic lead.  Plus the sho...  Agreed, Ioan Gruffudd intro's and outro's for ...\n",
       "8   I don't know about you, but I don't typically ...  I remember when I was in high school I had a h...\n",
       "9   I'm saying the people screaming racist on a mo...  Oh, I thought you were saying it the other way...\n",
       "10  :|==========כ newlinechar  newlinechar That lo...                          haha props for the effort\n",
       "11  Sure sure, but it's a first name and a low res...                     Doesn't matter. It's bad form.\n",
       "12  Hnnng Lundqvist. Great player, and uhm...is th...  [No, but there's this.](http://www.underwearex...\n",
       "13  You could learn welding and/or machining. Both...  Both of those are on the top of my list. I've ...\n",
       "14  I like using Spotify (be sure to use the web p...  For the windows app there are tools to auto mu...\n",
       "15   Actually he only said medical marijuana in 2001.  Very true, entirely my bad on implying it was ...\n",
       "16  Psst,  if you want him to win, try being more ...  My optimism was defeated when I hoped for chan...\n",
       "17  Kiper fucking had him to us at 9 in his latest...  Hahahaha...didn't see that. He gets paid to lo...\n",
       "18                Lol Gurley was NOT gonna fall to 23  I know, everyone had him all over their mocks,...\n",
       "19                   you can turn off subreddit style                                thats for puss-puss"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.read_table( 'file3.txt' , names=[ 'user1' , 'user2' ] )\n",
    "lines = lines.iloc[:] \n",
    "lines.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "mNjLtQyoswDg",
    "outputId": "c905da49-8c94-4981-84cd-f0d483b22938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user1 max length is 62\n",
      "Encoder input data shape -> (1500, 62)\n",
      "Number of user1 tokens = 5224\n"
     ]
    }
   ],
   "source": [
    "user1_lines = list()\n",
    "for line in lines.user1:\n",
    "    user1_lines.append( line ) \n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( user1_lines ) \n",
    "tokenized_user1_lines = tokenizer.texts_to_sequences( user1_lines ) \n",
    "\n",
    "length_list = list()\n",
    "for token_seq in tokenized_user1_lines:\n",
    "    length_list.append( len( token_seq ))\n",
    "max_input_length = np.array( length_list ).max()\n",
    "print( 'user1 max length is {}'.format( max_input_length ))\n",
    "\n",
    "padded_user1_lines = preprocessing.sequence.pad_sequences( tokenized_user1_lines , maxlen=max_input_length , padding='post' )\n",
    "encoder_input_data = np.array( padded_user1_lines )\n",
    "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n",
    "\n",
    "user1_word_dict = tokenizer.word_index\n",
    "num_user1_tokens = len( user1_word_dict )+1\n",
    "print( 'Number of user1 tokens = {}'.format( num_user1_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jVoiJ5kTtn5Q",
    "outputId": "8704bb2c-ec5c-45dc-bbb1-00bed2e703a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user2 max length is 64\n",
      "Decoder input data shape -> (1500, 64)\n",
      "Number of user2 tokens = 5026\n"
     ]
    }
   ],
   "source": [
    "user2_lines = list()\n",
    "for line in lines.user2:\n",
    "    user2_lines.append( '<START> ' + line + ' <END>' )  \n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( user2_lines ) \n",
    "tokenized_user2_lines = tokenizer.texts_to_sequences( user2_lines ) \n",
    "\n",
    "length_list = list()\n",
    "for token_seq in tokenized_user2_lines:\n",
    "    length_list.append( len( token_seq ))\n",
    "max_output_length = np.array( length_list ).max()\n",
    "print( 'user2 max length is {}'.format( max_output_length ))\n",
    "\n",
    "padded_user2_lines = preprocessing.sequence.pad_sequences( tokenized_user2_lines , maxlen=max_output_length, padding='post' )\n",
    "decoder_input_data = np.array( padded_user2_lines )\n",
    "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
    "\n",
    "user2_word_dict = tokenizer.word_index\n",
    "num_user2_tokens = len( user2_word_dict )+1\n",
    "print( 'Number of user2 tokens = {}'.format( num_user2_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q71oBgsFtoGW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_7Etzl60toR2",
    "outputId": "c7feea33-043a-44a2-f858-569f0bf74241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder target data shape -> (1500, 64, 5026)\n"
     ]
    }
   ],
   "source": [
    "decoder_target_data = list()\n",
    "for token_seq in tokenized_user2_lines:\n",
    "    decoder_target_data.append( token_seq[ 1 : ] ) \n",
    "    \n",
    "padded_user2_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
    "onehot_user2_lines = utils.to_categorical( padded_user2_lines , num_user2_tokens )\n",
    "decoder_target_data = np.array( onehot_user2_lines )\n",
    "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPx9ogIxuAM4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "hviInJhyuAas",
    "outputId": "b299670e-91c9-4160-a455-dfd0a801aadb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    1337344     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    1286656     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 197120      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  197120      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 5026)   648354      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,666,594\n",
      "Trainable params: 3,666,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( num_user1_tokens, 256 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 128 , return_state=True  )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( num_user2_tokens, 256 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 128 , return_state=True , return_sequences=True)\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( num_user2_tokens , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "filepath = \"model.h5\"\n",
    "checkpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint1]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3cQBO3Rcujft",
    "outputId": "9fd14a25-fe76-456f-b828-b6f46348121c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1500 samples\n",
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 41s 27ms/sample - loss: 2.2246\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 37s 25ms/sample - loss: 1.8731\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.7373\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.7193\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.7131\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.7079\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.7025\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.6957\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 37s 25ms/sample - loss: 1.6867\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.6774\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.6666\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.6575\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.6483\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.6395\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.6304\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 37s 25ms/sample - loss: 1.6216\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.6138\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.6056\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5979\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5903\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5831\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5759\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5687\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5624\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5548\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5486\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5416\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.5350\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5289\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5217\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.5157\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 37s 25ms/sample - loss: 1.5091\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.5027\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.4965\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 37s 24ms/sample - loss: 1.4900\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4841\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4798\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4713\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4645\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4585\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4519\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4463\n",
      "Epoch 43/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4390\n",
      "Epoch 44/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4327\n",
      "Epoch 45/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4275\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4197\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4130\n",
      "Epoch 48/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4076\n",
      "Epoch 49/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.4005\n",
      "Epoch 50/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3942\n",
      "Epoch 51/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3873\n",
      "Epoch 52/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3821\n",
      "Epoch 53/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3747\n",
      "Epoch 54/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3682\n",
      "Epoch 55/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3616\n",
      "Epoch 56/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3550\n",
      "Epoch 57/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3484\n",
      "Epoch 58/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3418\n",
      "Epoch 59/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3356\n",
      "Epoch 60/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3325\n",
      "Epoch 61/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3236\n",
      "Epoch 62/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3164\n",
      "Epoch 63/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3102\n",
      "Epoch 64/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.3037\n",
      "Epoch 65/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2975\n",
      "Epoch 66/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2921\n",
      "Epoch 67/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2846\n",
      "Epoch 68/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2777\n",
      "Epoch 69/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2721\n",
      "Epoch 70/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2652\n",
      "Epoch 71/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2587\n",
      "Epoch 72/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2515\n",
      "Epoch 73/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2451\n",
      "Epoch 74/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2380\n",
      "Epoch 75/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2319\n",
      "Epoch 76/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2233\n",
      "Epoch 77/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2174\n",
      "Epoch 78/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2126\n",
      "Epoch 79/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.2041\n",
      "Epoch 80/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1965\n",
      "Epoch 81/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1902\n",
      "Epoch 82/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1834\n",
      "Epoch 83/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1775\n",
      "Epoch 84/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1695\n",
      "Epoch 85/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1636\n",
      "Epoch 86/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1558\n",
      "Epoch 87/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1494\n",
      "Epoch 88/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1433\n",
      "Epoch 89/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1369\n",
      "Epoch 90/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1290\n",
      "Epoch 91/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1222\n",
      "Epoch 92/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1157\n",
      "Epoch 93/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1079\n",
      "Epoch 94/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.1023\n",
      "Epoch 95/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.0969\n",
      "Epoch 96/100\n",
      "1500/1500 [==============================] - 36s 24ms/sample - loss: 1.0874\n",
      "Epoch 97/100\n",
      "1500/1500 [==============================] - 35s 24ms/sample - loss: 1.0818\n",
      "Epoch 98/100\n",
      "1500/1500 [==============================] - 35s 24ms/sample - loss: 1.0769\n",
      "Epoch 99/100\n",
      "1500/1500 [==============================] - 35s 24ms/sample - loss: 1.0671\n",
      "Epoch 100/100\n",
      "1500/1500 [==============================] - 35s 24ms/sample - loss: 1.0609\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=250, epochs=50 ) \n",
    "model.save( 'model.h5' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfSGblw4ujrp"
   },
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 128 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 128 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NIv84x2Euj1s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQ390W19uj0B"
   },
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( user1_word_dict[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsoTkcHEujx9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "ruDbXDipujvZ",
    "outputId": "a0f0077d-1151-44c3-deaf-d6d3188340dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter user1 sentence : hi\n",
      " end\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4132920f8c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter user1 sentence : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# states_values = enc_model.predict( encoder_input_data[ epoch ] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for epoch in range( encoder_input_data.shape[0] ):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter user1 sentence : ' ) ) )\n",
    "    # states_values = enc_model.predict( encoder_input_data[ epoch ] )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = user2_word_dict['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in user2_word_dict.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArWRiKxqujpO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
